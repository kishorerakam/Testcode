import os
import re
import sqlparse
from collections import defaultdict
from sqlparse.sql import Identifier, IdentifierList
from sqlparse.tokens import Keyword, DML


def preprocess_snowflake_sql_for_sqlparse(sql_text: str) -> str:
    """
    Preprocess Snowflake SQL before passing to sqlparse.
    """
    sql_text = re.sub(r'\$\{(\w+)\}', r'DUMMY_\1', sql_text)  # Metadata variables
    sql_text = re.sub(r'--.*?$', '', sql_text, flags=re.MULTILINE)
    sql_text = re.sub(r'/\*.*?\*/', '', sql_text, flags=re.DOTALL)
    sql_text = re.sub(r'\bQUALIFY\b', '-- QUALIFY', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\b(COPY\s+INTO|PUT|GET|LIST|UNLOAD)\b.*?;', '', sql_text, flags=re.IGNORECASE | re.DOTALL)
    sql_text = re.sub(r'(\w+)(:\w+)+::(\w+)', lambda m: m.group(0).replace(':', '_').replace('::', '_'), sql_text)
    sql_text = re.sub(r'@\S+', 'STAGE_REFERENCE', sql_text)
    sql_text = re.sub(r'\bSAMPLE\s*\(.*?\)', '', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bWITH\s+RECURSIVE\b', 'WITH', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'FILE_FORMAT\s*=\s*\(.*?\)', '', sql_text, flags=re.IGNORECASE | re.DOTALL)
    sql_text = re.sub(r'\bMATCH_RECOGNIZE\b.*?(\))', '-- MATCH_RECOGNIZE_REMOVED', sql_text, flags=re.IGNORECASE | re.DOTALL)
    sql_text = re.sub(r'\bFLATTEN\s*\(.*?\)', 'FLATTEN_DUMMY', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bARRAY_AGG\s*\(', 'ARRAY_AGG_DUMMY(', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bOBJECT_CONSTRUCT\s*\(', 'OBJECT_CONSTRUCT_DUMMY(', sql_text, flags=re.IGNORECASE)
    sql_text = '\n'.join(line.strip() for line in sql_text.splitlines() if line.strip())
    return sql_text


def extract_lineage_from_tokens(parsed_statements):
    """
    Extract sources, targets, and CTEs from parsed SQL tokens.
    """
    sources = set()
    targets = set()
    ctes = set()

    for statement in parsed_statements:
        tokens = [token for token in statement.tokens if not token.is_whitespace]
        prev_token_value = None

        for token in tokens:
            if token.is_group:
                sub_sources, sub_targets, sub_ctes = extract_lineage_from_tokens([token])
                sources.update(sub_sources)
                targets.update(sub_targets)
                ctes.update(sub_ctes)

            if token.ttype is Keyword:
                token_value_upper = token.value.upper()

                if token_value_upper == "WITH":
                    next_token = _get_next_identifier(tokens, token)
                    if next_token:
                        ctes.add(next_token.get_real_name().upper())

                elif token_value_upper == "FROM" or token_value_upper == "JOIN":
                    next_token = _get_next_identifier(tokens, token)
                    if next_token:
                        sources.add(next_token.get_real_name().upper())

                elif token_value_upper == "INTO" and prev_token_value == "INSERT":
                    next_token = _get_next_identifier(tokens, token)
                    if next_token:
                        targets.add(next_token.get_real_name().upper())

            if token.ttype is DML:
                prev_token_value = token.value.upper()

    return sources, targets, ctes


def _get_next_identifier(tokens, current_token):
    """
    Helper to find the next Identifier token after a given token.
    """
    found = False
    for token in tokens:
        if found:
            if isinstance(token, Identifier):
                return token
            elif isinstance(token, IdentifierList):
                for identifier in token.get_identifiers():
                    return identifier
        if token == current_token:
            found = True
    return None


def process_sql_files_from_folders(folder_paths):
    """
    Process SQL files from the specified folders and build lineage graph.
    """
    lineage_graph = defaultdict(list)

    for folder_path in folder_paths:
        print(f"üìÇ Processing folder: {folder_path}")
        if not os.path.exists(folder_path):
            print(f"‚ö†Ô∏è Folder does not exist: {folder_path}")
            continue

        for file in os.listdir(folder_path):
            if file.endswith('.sql'):
                full_path = os.path.join(folder_path, file)
                print(f"üîµ Reading file: {full_path}")
                with open(full_path, 'r', encoding='utf-8') as f:
                    sql_text = f.read()

                cleaned_sql = preprocess_snowflake_sql_for_sqlparse(sql_text)
                parsed_statements = sqlparse.parse(cleaned_sql)
                sources, targets, ctes = extract_lineage_from_tokens(parsed_statements)

                # Build lineage edges
                for target in targets:
                    for source in sources.union(ctes):
                        lineage_graph[source].append(target)

    return lineage_graph


def display_lineage_graph(lineage_graph):
    """
    Display the birth-to-end lineage graph.
    """
    print("\nüîó Full Lineage Flow (Source ‚Üí Target):")
    for source, targets in lineage_graph.items():
        for target in targets:
            print(f"{source} ‚Üí {target}")


if __name__ == "__main__":
    # üëâ Put your two folder paths here
    folder_paths = [
        '/path/to/first/folder',   # <-- change this
        '/path/to/second/folder'   # <-- change this
    ]

    lineage_graph = process_sql_files_from_folders(folder_paths)
    display_lineage_graph(lineage_graph)
