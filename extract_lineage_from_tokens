import sqlparse
from sqlparse.sql import Identifier, IdentifierList
from sqlparse.tokens import Keyword, DML

def extract_lineage_from_tokens(parsed_statements):
    """
    Extract Source Tables, Target Tables, and CTEs from parsed sqlparse tokens.
    """
    sources = set()
    targets = set()
    ctes = set()

    for statement in parsed_statements:
        tokens = [token for token in statement.tokens if not token.is_whitespace]
        prev_token_value = None

        for token in tokens:
            if token.is_group:
                # If token is a group (subquery, etc.), recursively process
                sub_sources, sub_targets, sub_ctes = extract_lineage_from_tokens([token])
                sources.update(sub_sources)
                targets.update(sub_targets)
                ctes.update(sub_ctes)

            if token.ttype is Keyword:
                token_value_upper = token.value.upper()

                if token_value_upper == "WITH":
                    # After WITH, expect CTE identifiers
                    next_token = _get_next_identifier(tokens, token)
                    if next_token:
                        ctes.add(next_token.get_real_name().upper())

                elif token_value_upper == "FROM" or token_value_upper == "JOIN":
                    # After FROM/JOIN, expect source table
                    next_token = _get_next_identifier(tokens, token)
                    if next_token:
                        sources.add(next_token.get_real_name().upper())

                elif token_value_upper == "INTO" and prev_token_value == "INSERT":
                    # INSERT INTO target_table
                    next_token = _get_next_identifier(tokens, token)
                    if next_token:
                        targets.add(next_token.get_real_name().upper())

            if token.ttype is DML:
                prev_token_value = token.value.upper()

    return {
        'sources': sources,
        'targets': targets,
        'ctes': ctes
    }

def _get_next_identifier(tokens, current_token):
    """
    Helper to find the next Identifier token after a given token.
    """
    found = False
    for token in tokens:
        if found:
            if isinstance(token, Identifier):
                return token
            elif isinstance(token, IdentifierList):
                # In case multiple tables are listed
                for identifier in token.get_identifiers():
                    return identifier
        if token == current_token:
            found = True
    return None
